可控扩散模型（**Controllable Diffusion Models**）是对传统扩散模型（Diffusion Models）的一种扩展，它的核心目的是：**在生成图像的过程中加入某种“控制条件”，从而使生成的结果更符合用户需求或输入的引导**。

---

## 🧠 一、基础回顾：普通的 UNet 扩散模型是干嘛的？

传统扩散模型的原理：

1. **正向扩散过程（Forward Process）：**  
   把图像逐渐加入噪声，直到接近纯高斯噪声。

2. **反向生成过程（Reverse Process）：**  
   从噪声一步步去噪，最终还原出图像。  
   - 这个过程通过一个神经网络（通常是 **UNet**）来学习噪声的分布。

> **UNet**：在扩散模型中，UNet 是核心的神经网络结构，用来预测每一步噪声值，从而一步步“反扩散”出清晰图像。

---

## 🎮 二、可控扩散模型的核心思想是什么？

普通扩散模型只能随机生成图像（或者最多用一个类别标签指导，比如“生成一只狗”）。

**可控扩散模型加入了控制条件（condition）**，比如：

- 文本（文本生成图像）  
- 草图（线稿变照片）  
- 边缘图、人体姿态、深度图、分割图等等

这样就可以实现“有引导”的生成。

---

## 🔬 三、可控扩散模型 vs 普通扩散模型（UNet）

| 特点 | 普通 UNet 扩散模型 | 可控扩散模型 |
|------|-------------------|-------------|
| 输入 | 纯噪声 + 时间步t | 噪声 + 时间步t + 控制条件 |
| 输出 | 图像（无特定结构） | 图像（受控制结构影响） |
| 结构 | 标准 UNet | 在UNet结构中加入 control 分支，或使用 ControlNet |
| 应用 | 随机图像生成 | 文生图、草图生成、图像编辑等 |

---

## 🔧 四、ControlNet：一种流行的可控扩散模型结构

**ControlNet** 是可控扩散模型中的一个典型代表，它的特点是：

- **冻结主UNet**（主扩散模型结构不变）；
- **添加一个支路网络**，输入控制条件（比如边缘图），对 UNet 的特征进行引导；
- 控制分支学习条件与图像之间的关系，然后通过融合层影响主UNet的输出。

> 类比一下：  
> 普通 UNet 就像一个画画的机器人，可控 UNet（加了 ControlNet）就是你告诉它“你要画一只狗，还要躺着”，它就按你的描述画。

---

## 🧪 五、总结一句话：

> **可控扩散模型 = 原始UNet扩散模型 + 控制条件的输入 + 调整网络结构以融合这些条件（比如使用ControlNet）**

它的目标就是：**让AI“听你话”地生成图像**。

---

如果你想看个简单例子或者代码对比，我也可以列出来~ 想了解哪个方向的可控扩散模型？比如文字、草图、人体姿态图？